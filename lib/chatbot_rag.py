# RAG-Enhanced Medical Chatbot API
# This replaces the simple Gemini-only chatbot with a RAG system

from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
import re
import os
import warnings
import time
import base64
import io
from typing import List, Dict, Any
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
import pyarabic.araby as araby
from PIL import Image
from werkzeug.utils import secure_filename

# Suppress warnings
warnings.filterwarnings('ignore')

app = Flask(__name__)
CORS(app)

# Configure upload settings
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}
MAX_FILE_SIZE = 16 * 1024 * 1024  # 16MB max file size

# Create upload folder if it doesn't exist
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = MAX_FILE_SIZE

# Initialize Gemini model
API_KEY = "AIzaSyAmsXTtQjGlBMPthUoykwKQUeA3DLxqspE"
genai.configure(api_key=API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

class MedicalRAGChatbot:
    def __init__(self, train_csv_path="../RAG/train.csv"):
        self.vectorizer = None
        self.embeddings = None
        self.documents = []
        self.metadata = []
        self.train_csv_path = train_csv_path
        
        # Load and initialize the RAG system
        self.load_data()
        self.create_embeddings()
    
    def allowed_file(self, filename):
        """Check if the uploaded file has an allowed extension"""
        return '.' in filename and \
               filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS
    
    def process_image_with_gemini(self, image, user_prompt=""):
        """Process image using Gemini Vision model"""
        try:
            # If no specific prompt provided, use default medical analysis prompt
            if not user_prompt or user_prompt.strip() == "":
                prompt = """Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© ÙˆÙ‚Ø¯Ù…:

1. ÙˆØµÙ Ù…Ø§ ØªØ±Ø§Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
2. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© (Ø£Ø´Ø¹Ø©ØŒ ØªØ­Ù„ÙŠÙ„ØŒ Ø¯ÙˆØ§Ø¡ØŒ Ø¥Ù„Ø®) - Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„ Ø·Ø¨ÙŠ Ù…ÙØµÙ„
3. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØµÙˆØ±Ø© Ø¯ÙˆØ§Ø¡ - Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§ØªÙ‡
4. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£Ø´Ø¹Ø© Ø£Ùˆ ÙØ­Øµ Ø·Ø¨ÙŠ - Ø­Ø¯Ø¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø·Ø¨ÙŠØ¹ÙŠØ© Ø£Ù… Ù„Ø§ ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ø¨
5. Ù‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø·Ø¨ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ø¥Ù† Ø£Ù…ÙƒÙ†

Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨."""
            else:
                prompt = f"""Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ. Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ³Ø£Ù„: {user_prompt}

Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØ£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ ÙˆÙ…ÙÙŠØ¯."""

            # Generate content using Gemini with image
            response = gemini_model.generate_content([prompt, image])
            
            if response.text:
                return self.clean_response(response.text)
            else:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
                
        except Exception as e:
            print(f"Error processing image with Gemini: {e}")
            return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}"
    
    def analyze_image_from_base64(self, base64_string, user_prompt=""):
        """Analyze image from base64 string"""
        try:
            # Remove data URL prefix if present
            if base64_string.startswith('data:image'):
                base64_string = base64_string.split(',')[1]
            
            # Decode base64 to image
            image_data = base64.b64decode(base64_string)
            image = Image.open(io.BytesIO(image_data))
            
            # Process with Gemini
            return self.process_image_with_gemini(image, user_prompt)
            
        except Exception as e:
            print(f"Error analyzing image from base64: {e}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ ØµØ­ÙŠØ­."
    
    def analyze_image_from_file(self, file_path, user_prompt=""):
        """Analyze image from file path"""
        try:
            # Open and process the image
            image = Image.open(file_path)
            
            # Process with Gemini
            return self.process_image_with_gemini(image, user_prompt)
            
        except Exception as e:
            print(f"Error analyzing image from file: {e}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø¯."

    def normalize_arabic(self, text):
        """Normalize Arabic text by standardizing various forms of letters."""
        if not isinstance(text, str):
            return ""
            
        # Standardize alef forms
        text = re.sub("[Ø¥Ø£Ù±Ø¢Ø§]", "Ø§", text)
        # Standardize ya forms
        text = re.sub("Ù‰", "ÙŠ", text)
        # Standardize hamza forms (but preserve Ø¦ as it has different meaning)
        text = re.sub("Ø¤", "Ø¡", text)
        
        # Remove diacritics (tashkeel)
        text = araby.strip_tashkeel(text)
        
        return text

    def preprocess_text(self, text):
        """Enhanced preprocessing function for Arabic and English text"""
        if not isinstance(text, str):
            return ""
        
        # Apply Arabic normalization
        text = self.normalize_arabic(text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # For better TF-IDF, keep some punctuation but remove others
        text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]', ' ', text)
        
        return text

    def load_data(self):
        """Load the training dataset"""
        try:
            print(f"Loading data from: {self.train_csv_path}")
            
            # Try different possible paths
            possible_paths = [
                self.train_csv_path,
                "RAG/train.csv",
                "../RAG/train.csv",
                "train.csv"
            ]
            
            df = None
            for path in possible_paths:
                if os.path.exists(path):
                    df = pd.read_csv(path)
                    print(f"âœ“ Data loaded from: {path}")
                    break
            
            if df is None:
                raise FileNotFoundError("Could not find train.csv file")
            
            # Preprocess the data
            df['question'] = df['question'].apply(self.preprocess_text)
            df['answer'] = df['answer'].apply(self.preprocess_text)
            
            # Create documents combining question and answer
            for _, row in df.iterrows():
                doc_text = f"Question: {row['question']}\nAnswer: {row['answer']}\nCategory: {row['label']}"
                self.documents.append(doc_text)
                self.metadata.append({
                    'question': row['question'],
                    'answer': row['answer'],
                    'label': row['label']
                })
            
            print(f"âœ“ Loaded {len(self.documents)} medical documents")
            
        except Exception as e:
            print(f"Error loading data: {e}")
            # Fallback with sample data
            self.create_fallback_data()

    def create_fallback_data(self):
        """Create comprehensive fallback data if CSV loading fails"""
        fallback_data = [
            {
                'question': 'Ù…Ø§ Ù‡ÙŠ Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø³ÙƒØ±ÙŠØŸ',
                'answer': 'Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø³ÙƒØ±ÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ø¹Ø·Ø´ Ø§Ù„Ø´Ø¯ÙŠØ¯ØŒ ÙƒØ«Ø±Ø© Ø§Ù„ØªØ¨ÙˆÙ„ Ø®Ø§ØµØ© Ù„ÙŠÙ„Ø§Ù‹ØŒ Ø§Ù„ØªØ¹Ø¨ ÙˆØ§Ù„Ø¥Ø±Ù‡Ø§Ù‚ Ø§Ù„Ù…Ø³ØªÙ…Ø±ØŒ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ÙˆØ²Ù† ØºÙŠØ± Ø§Ù„Ù…Ø¨Ø±Ø±ØŒ Ø§Ù„Ø¬ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø·ØŒ Ø¨Ø·Ø¡ Ø´ÙØ§Ø¡ Ø§Ù„Ø¬Ø±ÙˆØ­ØŒ ØªØ´ÙˆØ´ Ø§Ù„Ø±Ø¤ÙŠØ©ØŒ ÙˆØ§Ù„Ø§Ù„ØªÙ‡Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ø®Ø§ØµØ© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ù„Ùƒ Ø§Ù„Ø¨ÙˆÙ„ÙŠØ©.',
                'label': 'Diabetes'
            },
            {
                'question': 'ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¹Ù„Ø§Ø¬ Ø§Ø±ØªÙØ§Ø¹ Ø¶ØºØ· Ø§Ù„Ø¯Ù…ØŸ',
                'answer': 'Ø¹Ù„Ø§Ø¬ Ø§Ø±ØªÙØ§Ø¹ Ø¶ØºØ· Ø§Ù„Ø¯Ù… ÙŠØ´Ù…Ù„ ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ù†Ù…Ø· Ø§Ù„Ø­ÙŠØ§Ø© Ù…Ø«Ù„ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ø­ ÙÙŠ Ø§Ù„Ø·Ø¹Ø§Ù…ØŒ Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø§Ù†ØªØ¸Ø§Ù…ØŒ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ÙˆØ²Ù† ØµØ­ÙŠØŒ ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¯Ø®ÙŠÙ† ÙˆØ§Ù„ÙƒØ­ÙˆÙ„ØŒ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙˆØªØ±ØŒ ÙˆØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø¶ØºØ· Ø­Ø³Ø¨ ÙˆØµÙØ© Ø§Ù„Ø·Ø¨ÙŠØ¨.',
                'label': 'Hypertension'
            },
            {
                'question': 'What are the symptoms of flu?',
                'answer': 'Flu symptoms include sudden onset of fever (usually high), chills and sweats, severe body aches and muscle pain, fatigue and weakness, dry persistent cough, sore throat, runny or stuffy nose, headache, and sometimes nausea and vomiting.',
                'label': 'Influenza'
            },
            {
                'question': 'Ù…Ø§ Ù‡ÙŠ Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„ØµØ¯Ø§Ø¹ Ø§Ù„Ù†ØµÙÙŠØŸ',
                'answer': 'Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„ØµØ¯Ø§Ø¹ Ø§Ù„Ù†ØµÙÙŠ ØªØ´Ù…Ù„ Ø£Ù„Ù… Ø´Ø¯ÙŠØ¯ ÙÙŠ Ø¬Ø§Ù†Ø¨ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø±Ø£Ø³ØŒ Ø§Ù„ØºØ«ÙŠØ§Ù† ÙˆØ§Ù„Ù‚ÙŠØ¡ØŒ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¶ÙˆØ¡ ÙˆØ§Ù„ØµÙˆØªØŒ ØªØ´ÙˆØ´ Ø§Ù„Ø±Ø¤ÙŠØ© Ø£Ùˆ Ø±Ø¤ÙŠØ© Ø£Ø¶ÙˆØ§Ø¡ØŒ ÙˆÙ‚Ø¯ ØªØ³Ø¨Ù‚ Ø§Ù„Ù†ÙˆØ¨Ø© Ø£Ø¹Ø±Ø§Ø¶ ØªØ­Ø°ÙŠØ±ÙŠØ© Ù…Ø«Ù„ ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ø¬ Ø£Ùˆ Ø§Ù„Ø´Ù‡ÙŠØ©.',
                'label': 'Migraine'
            },
            {
                'question': 'ÙƒÙŠÙ Ø£Ø¹Ø§Ù„Ø¬ Ù†Ø²Ù„Ø© Ø§Ù„Ø¨Ø±Ø¯ØŸ',
                'answer': 'Ø¹Ù„Ø§Ø¬ Ù†Ø²Ù„Ø© Ø§Ù„Ø¨Ø±Ø¯ ÙŠØ´Ù…Ù„ Ø§Ù„Ø±Ø§Ø­Ø© Ø§Ù„ØªØ§Ù…Ø©ØŒ Ø´Ø±Ø¨ Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ø§Ù„Ø¯Ø§ÙØ¦Ø© Ø¨ÙƒØ«Ø±Ø©ØŒ Ø§Ù„ØºØ±ØºØ±Ø© Ø¨Ø§Ù„Ù…Ø§Ø¡ Ø§Ù„Ù…Ø§Ù„Ø­ Ù„Ù„Ø­Ù„Ù‚ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø·Ø±Ø§Øª Ø§Ù„Ø£Ù†Ù Ø§Ù„Ù…Ø§Ù„Ø­Ø©ØŒ ØªÙ†Ø§ÙˆÙ„ ÙÙŠØªØ§Ù…ÙŠÙ† Ø³ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø±Ø·Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¡ØŒ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‡ÙŠØ¬Ø§Øª Ù…Ø«Ù„ Ø§Ù„Ø¯Ø®Ø§Ù†.',
                'label': 'Common Cold'
            },
            {
                'question': 'What causes back pain?',
                'answer': 'Back pain can be caused by muscle strain from heavy lifting or sudden movements, poor posture, herniated discs, arthritis, osteoporosis, kidney problems, or stress. Most back pain is mechanical and improves with rest, gentle exercise, and proper ergonomics.',
                'label': 'Back Pain'
            },
            {
                'question': 'Ù…Ø§ Ù‡ÙŠ Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ù…ÙØ§ØµÙ„ØŸ',
                'answer': 'Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ù…ÙØ§ØµÙ„ ØªØ´Ù…Ù„ Ø£Ù„Ù… ÙˆØªÙˆØ±Ù… ÙÙŠ Ø§Ù„Ù…ÙØ§ØµÙ„ØŒ ØªÙŠØ¨Ø³ Ø®Ø§ØµØ© ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ØŒ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ø­Ø±ÙƒØ©ØŒ Ø§Ø­Ù…Ø±Ø§Ø± ÙˆØ¯ÙØ¡ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…ØµØ§Ø¨Ø©ØŒ ÙˆÙ‚Ø¯ ÙŠØµØ§Ø­Ø¨Ù‡Ø§ ØªØ¹Ø¨ Ø¹Ø§Ù… ÙˆØ­Ù…Ù‰ Ø®ÙÙŠÙØ© ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹.',
                'label': 'Arthritis'
            },
            {
                'question': 'ÙƒÙŠÙ Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ',
                'answer': 'Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±: Ù…Ø§Ø±Ø³ ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„ØªÙ†ÙØ³ Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŒ Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙˆÙ… Ø§Ù„ÙƒØ§ÙÙŠØŒ Ù…Ø§Ø±Ø³ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø¨Ø§Ù†ØªØ¸Ø§Ù…ØŒ ØªØ¬Ù†Ø¨ Ø§Ù„ÙƒØ§ÙÙŠÙŠÙ† Ø§Ù„Ø²Ø§Ø¦Ø¯ØŒ Ù…Ø§Ø±Ø³ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ ÙˆØ§Ù„ØªØ£Ù…Ù„ØŒ ØªØ­Ø¯Ø« Ù…Ø¹ Ø£Ø´Ø®Ø§Øµ ØªØ«Ù‚ Ø¨Ù‡Ù…ØŒ ÙˆØ¥Ø°Ø§ Ø§Ø³ØªÙ…Ø± Ø§Ù„Ù‚Ù„Ù‚ Ø§Ø³ØªØ´Ø± Ù…Ø®ØªØµ Ù†ÙØ³ÙŠ.',
                'label': 'Anxiety'
            }
        ]
        
        for item in fallback_data:
            # Make the document more searchable by including keywords
            doc_text = f"Question: {item['question']}\nAnswer: {item['answer']}\nCategory: {item['label']}\nKeywords: medical health symptoms treatment"
            self.documents.append(doc_text)
            self.metadata.append(item)
        
        print(f"âœ“ Using {len(fallback_data)} comprehensive medical fallback documents")

    def create_embeddings(self):
        """Create TF-IDF embeddings for the documents"""
        try:
            print("Creating embeddings...")
            
            # Use TF-IDF vectorizer with better settings for Arabic and English
            self.vectorizer = TfidfVectorizer(
                max_features=2000,  # Increased from 1000
                ngram_range=(1, 3),  # Include trigrams for better context
                lowercase=True,
                stop_words=None,  # Handle both Arabic and English
                min_df=1,  # Include terms that appear at least once
                max_df=0.95,  # Exclude very common terms
                sublinear_tf=True,  # Use sublinear TF scaling
                analyzer='word'
            )
            
            # Fit and transform documents
            self.embeddings = self.vectorizer.fit_transform(self.documents)
            print(f"âœ“ Created embeddings with shape: {self.embeddings.shape}")
            
            # Debug: Print vocabulary size
            vocab_size = len(self.vectorizer.vocabulary_)
            print(f"âœ“ Vocabulary size: {vocab_size}")
            
        except Exception as e:
            print(f"Error creating embeddings: {e}")

    def retrieve_relevant_documents(self, query, k=3):
        """Retrieve relevant documents for a query"""
        try:
            if self.vectorizer is None or self.embeddings is None:
                return []
            
            # Preprocess query
            processed_query = self.preprocess_text(query)
            
            # Transform query to vector
            query_vector = self.vectorizer.transform([processed_query])
            
            # Calculate cosine similarity
            similarities = cosine_similarity(query_vector, self.embeddings).flatten()
            
            # Get top k documents
            top_indices = similarities.argsort()[-k:][::-1]
            
            relevant_docs = []
            
            # More lenient similarity threshold for better retrieval
            threshold = 0.1  # Increased from 0.05
            max_similarity = similarities.max() if len(similarities) > 0 else 0
            
            print(f"ğŸ” Query: '{query[:50]}...'")
            print(f"ğŸ“Š Max similarity found: {max_similarity:.3f}")
            
            # If max similarity is very low, reduce threshold adaptively
            if max_similarity < threshold:
                adaptive_threshold = max(0.01, max_similarity * 0.5)  # Use 50% of max similarity
                print(f"ğŸ“‰ Adaptive threshold: {adaptive_threshold:.3f}")
            else:
                adaptive_threshold = threshold
            
            # Get documents above adaptive threshold
            for idx in top_indices:
                if similarities[idx] >= adaptive_threshold:
                    relevant_docs.append({
                        'content': self.documents[idx],
                        'metadata': self.metadata[idx],
                        'similarity': similarities[idx]
                    })
                    print(f"âœ“ Retrieved doc {idx}: similarity={similarities[idx]:.3f}")
            
            # If still no documents found, take the best match if it's not completely irrelevant
            if not relevant_docs and len(top_indices) > 0:
                best_idx = top_indices[0]
                if similarities[best_idx] > 0.001:  # Very minimal threshold
                    relevant_docs.append({
                        'content': self.documents[best_idx],
                        'metadata': self.metadata[best_idx],
                        'similarity': similarities[best_idx]
                    })
                    print(f"ğŸ“ Using best available match: similarity={similarities[best_idx]:.3f}")
            
            if relevant_docs:
                print(f"âœ… Retrieved {len(relevant_docs)} relevant documents")
            else:
                print("âŒ No relevant documents found in knowledge base")
            
            return relevant_docs
            
        except Exception as e:
            print(f"Error retrieving documents: {e}")
            return []

    def generate_rag_response(self, query):
        """Generate response using RAG system - Retrieve documents first, then use Gemini"""
        try:
            # Step 1: Try to retrieve relevant documents
            relevant_docs = self.retrieve_relevant_documents(query, k=3)
            
            # Step 2: Determine response strategy based on document retrieval
            if relevant_docs and relevant_docs[0]['similarity'] > 0.1:
                # High confidence retrieval - use documents as primary context
                context = "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©:\n\n"
                
                for i, doc in enumerate(relevant_docs, 1):
                    answer = doc['metadata']['answer']
                    category = doc['metadata']['label']
                    similarity = doc['similarity']
                    context += f"{i}. {answer}\n[Ø§Ù„Ù…ØµØ¯Ø±: {category} - Ø§Ù„Ø¯Ù‚Ø©: {similarity:.2f}]\n\n"
                
                medical_prompt = f"""Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶.

{context}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø£Ø¹Ù„Ø§Ù‡ ÙƒØ£Ø³Ø§Ø³ Ù„Ø¥Ø¬Ø§Ø¨ØªÙƒ
2. ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ø¹Ø§Ù…Ø© Ù„ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
3. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
4. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ (Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
5. ÙƒÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ©

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:"""
                
                print(f"ğŸ¯ Using {len(relevant_docs)} high-confidence retrieved documents")
                
            elif relevant_docs and relevant_docs[0]['similarity'] > 0.05:
                # Medium confidence - use documents but supplement with general knowledge
                context = "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ù…Ø±Ø¬Ø¹ÙŠØ© (Ù‚Ø¯ ØªÙƒÙˆÙ† Ø°Ø§Øª ØµÙ„Ø©):\n\n"
                
                for i, doc in enumerate(relevant_docs, 1):
                    answer = doc['metadata']['answer']
                    category = doc['metadata']['label']
                    context += f"{i}. {answer}\n[Ù…Ù† ØªØ®ØµØµ: {category}]\n\n"
                
                medical_prompt = f"""Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ. Ù„Ø¯ÙŠÙƒ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© ÙˆØ®Ø¨Ø±ØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©.

{context}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø£Ø¹Ù„Ø§Ù‡ - Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø© Ø£Ùˆ Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
2. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© ÙƒÙ…ØµØ¯Ø± Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
3. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù…ÙÙŠØ¯Ø©ØŒ ÙØ§Ø¯Ù…Ø¬Ù‡Ø§ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ
4. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©ØŒ ÙØ§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ©
5. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø·Ø¨ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙÙŠØ¯Ø©

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:"""
                
                print(f"ğŸ” Using {len(relevant_docs)} medium-confidence documents + general knowledge")
                
            else:
                # Low or no confidence in retrieval - use Gemini's general medical knowledge
                medical_prompt = f"""Ø£Ù†Øª Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©.

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ§Ù„Ù…ØªØ®ØµØµØ©
2. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙÙŠØ¯Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©
3. Ø§Ø´Ø±Ø­ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ØŒ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ØŒ ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø°Ù„Ùƒ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹
4. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ (Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
5. ÙƒÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹ ÙÙŠ Ù†ØµØ§Ø¦Ø­Ùƒ Ø§Ù„Ø·Ø¨ÙŠØ©

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:"""
                
                print("ğŸ§  Using Gemini's general medical knowledge (no relevant documents found)")
            
            # Step 3: Generate response with Gemini
            response = self.gemini_generate(medical_prompt)
            
            # Step 4: Clean and validate response
            cleaned_response = self.clean_response(response)
            
            # If response is too short or empty, try fallback
            if not cleaned_response or len(cleaned_response.strip()) < 30:
                print("âš ï¸ Response too short, trying fallback approach")
                return self.generate_emergency_fallback(query)
            
            # Add appropriate medical disclaimer
            if relevant_docs and relevant_docs[0]['similarity'] > 0.1:
                disclaimer = "\n\nğŸ’¡ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ Ø¯Ù‚ÙŠÙ‚ ÙˆØ¹Ù„Ø§Ø¬ Ù…Ù†Ø§Ø³Ø¨ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ."
            else:
                disclaimer = "\n\nâš ï¸ ØªÙ†Ø¨ÙŠÙ‡ Ø·Ø¨ÙŠ: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨."
            
            return cleaned_response + disclaimer
            
        except Exception as e:
            print(f"âŒ Error generating response: {e}")
            return self.generate_emergency_fallback(query)

    def gemini_generate(self, prompt, max_retries=3):
        """Generate text using Gemini with retry logic"""
        for attempt in range(max_retries):
            try:
                response = gemini_model.generate_content(prompt)
                if response.text:
                    return response.text.strip()
                else:
                    print(f"Empty response from Gemini on attempt {attempt + 1}")
            except Exception as e:
                print(f"Gemini error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # Wait before retry
                else:
                    return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø© ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        
        return "ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª."

    def clean_response(self, response):
        """Clean the response by removing unwanted formatting while preserving medical content"""
        if not response:
            return ""
        
        # Remove markdown formatting but preserve content
        response = re.sub(r'\*\*([^*]+)\*\*', r'\1', response)  # Remove **bold**
        response = re.sub(r'\*([^*]+)\*', r'\1', response)      # Remove *italic*
        
        # Remove multiple asterisks
        response = re.sub(r'\*+', '', response)
        
        # Clean up excessive whitespace
        response = re.sub(r'\n\s*\n\s*\n', '\n\n', response)  # Remove triple newlines
        response = re.sub(r'\s+', ' ', response)  # Replace multiple spaces with single space
        response = response.strip()
        
        # Only reject responses that are clearly refusing to help
        # Be more lenient with medical responses
        refusal_patterns = [
            "Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªØ´Ø®ÙŠØµ",
            "Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠÙ… ØªØ´Ø®ÙŠØµ Ø·Ø¨ÙŠ",
            "Ø£Ø¹ØªØ°Ø±ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ",
            "ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ©"
        ]
        
        # Only reject if response is very short AND contains clear refusal
        if len(response) < 30 and any(pattern in response for pattern in refusal_patterns):
            return ""
        
        return response

    def generate_emergency_fallback(self, query):
        """Emergency fallback when Gemini fails - still try to use Gemini as medical assistant"""
        try:
            # More assertive medical assistant prompt as last resort
            simple_prompt = f"""Ø£Ù†Øª Ø·Ø¨ÙŠØ¨. ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠ:

{query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª:
- Ø£Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ù…ÙÙŠØ¯Ø©
- Ù„Ø§ ØªÙ‚Ù„ Ø£Ù†Ùƒ Ù„Ø§ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
- Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø©
- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©

Ø¥Ø¬Ø§Ø¨ØªÙƒ:"""
            
            response = self.gemini_generate(simple_prompt)
            if response and "ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø©" not in response and "Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªÙ‚Ø¯ÙŠÙ…" not in response:
                # Clean the response
                cleaned_response = self.clean_response(response)
                return cleaned_response + "\n\nâš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ù‡Ø°Ù‡ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø§Ù…Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ Ø¯Ù‚ÙŠÙ‚."
            else:
                # If Gemini completely fails, return static message with basic medical info
                return self.get_static_medical_response(query)
                
        except Exception as e:
            print(f"Error in emergency fallback: {e}")
            return self.get_static_medical_response(query)

    def get_static_medical_response(self, query):
        """Provide basic medical information when all AI fails"""
        query_lower = query.lower()
        
        # Basic medical responses for common questions
        if any(word in query_lower for word in ['Ø­Ù…Ù‰', 'fever', 'Ø­Ø±Ø§Ø±Ø©']):
            return """Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø­Ù…Ù‰ ØªØ´Ù…Ù„:
â€¢ Ø§Ø±ØªÙØ§Ø¹ Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ø¬Ø³Ù… ÙÙˆÙ‚ 38Â°Ù…
â€¢ Ø§Ù„Ù‚Ø´Ø¹Ø±ÙŠØ±Ø© ÙˆØ§Ù„Ø±Ø¹Ø´Ø©
â€¢ Ø§Ù„ØªØ¹Ø±Ù‚
â€¢ Ø§Ù„ØµØ¯Ø§Ø¹
â€¢ Ø¢Ù„Ø§Ù… Ø§Ù„Ø¹Ø¶Ù„Ø§Øª
â€¢ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø´Ù‡ÙŠØ©
â€¢ Ø§Ù„ØªØ¹Ø¨ ÙˆØ§Ù„Ø¥Ø±Ù‡Ø§Ù‚

Ø§Ù„Ø¹Ù„Ø§Ø¬:
â€¢ Ø§Ù„Ø±Ø§Ø­Ø© ÙˆØ´Ø±Ø¨ Ø§Ù„Ø³ÙˆØ§Ø¦Ù„
â€¢ Ø®Ø§ÙØ¶Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù…Ø«Ù„ Ø§Ù„Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„
â€¢ Ø§Ù„ÙƒÙ…Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø§Ø±Ø¯Ø©

âš ï¸ ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ø­Ù…Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 3 Ø£ÙŠØ§Ù… Ø£Ùˆ ØªØ¬Ø§ÙˆØ²Øª 39Â°Ù…."""
        
        elif any(word in query_lower for word in ['ØµØ¯Ø§Ø¹', 'headache', 'Ø±Ø£Ø³']):
            return """Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ØµØ¯Ø§Ø¹ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©:
â€¢ Ø§Ù„ØªÙˆØªØ± ÙˆØ§Ù„Ø¥Ø¬Ù‡Ø§Ø¯
â€¢ Ù‚Ù„Ø© Ø§Ù„Ù†ÙˆÙ…
â€¢ Ø§Ù„Ø¬ÙØ§Ù
â€¢ Ø§Ù„Ø¬ÙˆØ¹
â€¢ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¹ÙŠÙ†
â€¢ Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø¬ÙŠÙˆØ¨ Ø§Ù„Ø£Ù†ÙÙŠØ©

Ø§Ù„Ø¹Ù„Ø§Ø¬:
â€¢ Ø§Ù„Ø±Ø§Ø­Ø© ÙÙŠ Ù…ÙƒØ§Ù† Ù‡Ø§Ø¯Ø¦
â€¢ Ø´Ø±Ø¨ Ø§Ù„Ù…Ø§Ø¡
â€¢ Ù…Ø³ÙƒÙ†Ø§Øª Ø§Ù„Ø£Ù„Ù… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
â€¢ ØªØ·Ø¨ÙŠÙ‚ ÙƒÙ…Ø§Ø¯Ø§Øª Ø¨Ø§Ø±Ø¯Ø© Ø£Ùˆ Ø¯Ø§ÙØ¦Ø©

âš ï¸ Ø§Ø³ØªØ´Ø± Ø·Ø¨ÙŠØ¨ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØµØ¯Ø§Ø¹ Ø´Ø¯ÙŠØ¯ Ø£Ùˆ Ù…Ø³ØªÙ…Ø±."""
        
        else:
            return """Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø© ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø­Ø¯Ø¯Ø© Ù„Ø³Ø¤Ø§Ù„Ùƒ Ø­Ø§Ù„ÙŠØ§Ù‹.

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø©ØŒ ÙŠØ±Ø¬Ù‰:
1. Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ
2. Ø²ÙŠØ§Ø±Ø© Ø£Ù‚Ø±Ø¨ Ù…Ø±ÙƒØ² ØµØ­ÙŠ
3. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø· Ø§Ù„Ø³Ø§Ø®Ù† Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©

âš ï¸ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¹Ø§Ù… ÙˆÙ„ÙŠØ³ Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ©."""

    def get_static_fallback_response(self):
        """Static fallback when all else fails"""
        return """Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø© ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. 

ÙŠØ±Ø¬Ù‰:
1. Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰
2. Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©
3. Ø²ÙŠØ§Ø±Ø© Ø£Ù‚Ø±Ø¨ Ù…Ø±ÙƒØ² ØµØ­ÙŠ

âš ï¸ ØªØ°ÙƒÙŠØ±: Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø· ÙˆÙ„ÙŠØ³ Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ©."""

# Initialize the RAG chatbot
rag_chatbot = MedicalRAGChatbot()

# Flask routes
@app.route("/", methods=["GET"])
def home():
    return "âœ… RAG-Enhanced Medical Chatbot API is running!"

@app.route("/upload", methods=["GET"])
def upload_page():
    """Serve the image upload interface"""
    try:
        # Serve the HTML file for image upload
        html_path = os.path.join(os.path.dirname(__file__), 'image_upload_interface.html')
        if os.path.exists(html_path):
            with open(html_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            return """
            <h1>Image Upload Interface Not Found</h1>
            <p>The image upload interface file is missing. Please check that 'image_upload_interface.html' exists in the lib folder.</p>
            <p><a href="/health">Check Server Health</a></p>
            """
    except Exception as e:
        return f"<h1>Error</h1><p>Failed to load upload interface: {str(e)}</p>"

@app.route("/chat", methods=["POST", "GET"])
def chat():
    try:
        if request.method == "POST":
            data = request.get_json()
            user_input = data.get("message", "").strip()
            
            # Check if there's an image in the request
            image_data = data.get("image", "")
            if image_data:
                # Handle image analysis
                print("ğŸ–¼ï¸ Processing image with user query...")
                response = rag_chatbot.analyze_image_from_base64(image_data, user_input)
                return jsonify({"reply": response})
                
        else:  # GET method, for testing in browser
            user_input = request.args.get("message", "").strip()

        if not user_input:
            return jsonify({"reply": "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø³Ø§Ù„Ø©."}), 400

        # Handle greetings and gratitude
        user_lower = user_input.lower()
        
        if any(greeting in user_lower for greeting in ['hello', 'hi', 'Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…', 'Ø£Ù‡Ù„Ø§']):
            greeting_response = """Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø°ÙƒÙŠ! ğŸ‘‹

Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØ§Ù„ØµØ­ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù†:
â€¢ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙˆØ§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
â€¢ Ø§Ù„Ø¹Ù„Ø§Ø¬Ø§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆÙŠØ©
â€¢ Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„ØµØ­ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©
â€¢ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© (Ø£Ø´Ø¹Ø©ØŒ Ø£Ø¯ÙˆÙŠØ©ØŒ ÙØ­ÙˆØµØ§Øª)
â€¢ Ù…ØªÙ‰ ÙŠØ¬Ø¨ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"""
            return jsonify({"reply": greeting_response})
        
        if any(thanks in user_lower for thanks in ['thank you', 'thanks', 'Ø´ÙƒØ±Ø§', 'Ø´ÙƒØ±Ø§Ù‹']):
            thanks_response = "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø¯ÙˆØ§Ù… Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§ÙÙŠØ©. Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±. ğŸ˜Š"
            return jsonify({"reply": thanks_response})

        # Log the user input for debugging
        print(f"ğŸ” Processing query: {user_input}")
        
        # Always generate RAG-enhanced response using Gemini
        response = rag_chatbot.generate_rag_response(user_input)
        
        # Ensure we always have a response
        if not response or response.strip() == "":
            response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ."
        
        print(f"âœ… Generated response: {response[:100]}...")
        return jsonify({"reply": response})

    except Exception as e:
        print(f"Error in chat endpoint: {e}")
        error_response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        return jsonify({"reply": error_response}), 500

@app.route("/upload_image", methods=["POST"])
def upload_image():
    """Handle image upload via file upload"""
    try:
        # Check if image file is in the request
        if 'image' not in request.files:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±ÙØ§Ù‚ ØµÙˆØ±Ø©"}), 400
        
        file = request.files['image']
        user_prompt = request.form.get('prompt', '')
        
        # Check if file is selected
        if file.filename == '':
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù…Ù„Ù"}), 400
        
        # Check if file type is allowed
        if not rag_chatbot.allowed_file(file.filename):
            return jsonify({"error": f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: {', '.join(ALLOWED_EXTENSIONS)}"}), 400
        
        # Save the uploaded file
        filename = secure_filename(file.filename)
        timestamp = str(int(time.time()))
        filename = f"{timestamp}_{filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        print(f"ğŸ–¼ï¸ Image uploaded: {filename}")
        
        # Analyze the image
        response = rag_chatbot.analyze_image_from_file(file_path, user_prompt)
        
        # Clean up - remove the uploaded file after processing
        try:
            os.remove(file_path)
        except:
            pass  # Don't worry if file removal fails
        
        return jsonify({"reply": response})
        
    except Exception as e:
        print(f"Error in upload_image endpoint: {e}")
        return jsonify({"error": "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©"}), 500

@app.route("/analyze_image", methods=["POST"])
def analyze_image():
    """Handle image analysis via base64 data"""
    try:
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "ÙŠØ±Ø¬Ù‰ Ø¥Ø±ÙØ§Ù‚ ØµÙˆØ±Ø©"}), 400
        
        image_data = data.get('image', '')
        user_prompt = data.get('prompt', '')
        
        if not image_data:
            return jsonify({"error": "Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø© ÙØ§Ø±ØºØ©"}), 400
        
        print(f"ğŸ–¼ï¸ Analyzing image with prompt: {user_prompt[:50]}...")
        
        # Analyze the image
        response = rag_chatbot.analyze_image_from_base64(image_data, user_prompt)
        
        return jsonify({"reply": response})
        
    except Exception as e:
        print(f"Error in analyze_image endpoint: {e}")
        return jsonify({"error": "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©"}), 500

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "documents_loaded": len(rag_chatbot.documents),
        "embeddings_created": rag_chatbot.embeddings is not None,
        "image_analysis_enabled": True,
        "upload_folder": app.config['UPLOAD_FOLDER'],
        "allowed_extensions": list(ALLOWED_EXTENSIONS)
    })

@app.route("/debug", methods=["GET", "POST"])
def debug_retrieval():
    """Debug endpoint to test document retrieval"""
    try:
        if request.method == "POST":
            data = request.get_json()
            query = data.get("query", "").strip()
        else:
            query = request.args.get("query", "").strip()
        
        if not query:
            return jsonify({
                "error": "Please provide a query",
                "total_documents": len(rag_chatbot.documents),
                "sample_documents": [doc[:100] + "..." for doc in rag_chatbot.documents[:3]]
            })
        
        # Test document retrieval
        relevant_docs = rag_chatbot.retrieve_relevant_documents(query, k=5)
        
        result = {
            "query": query,
            "total_documents": len(rag_chatbot.documents),
            "retrieved_documents": len(relevant_docs),
            "documents": []
        }
        
        for doc in relevant_docs:
            result["documents"].append({
                "question": doc['metadata']['question'],
                "answer": doc['metadata']['answer'][:200] + "...",
                "category": doc['metadata']['label'],
                "similarity": round(doc['similarity'], 3)
            })
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": str(e)})

if __name__ == "__main__":
    print("ğŸš€ Starting RAG-Enhanced Medical Chatbot...")
    print(f"ğŸ“Š Loaded {len(rag_chatbot.documents)} medical documents")
    print("ğŸŒ Server starting on http://0.0.0.0:5000")
    print("\nğŸ“± Available endpoints:")
    print("   â€¢ Main API: http://localhost:5000")
    print("   â€¢ Image Upload Interface: http://localhost:5000/upload")
    print("   â€¢ Health Check: http://localhost:5000/health")
    print("   â€¢ Chat API: http://localhost:5000/chat")
    print("   â€¢ Debug: http://localhost:5000/debug")
    print("\nğŸ–¼ï¸ Image analysis features enabled!")
    print("   â€¢ Upload medical images via web interface")
    print("   â€¢ Support for X-rays, medicines, lab reports")
    print("   â€¢ Arabic and English analysis")
    app.run(debug=True, host='0.0.0.0', port=5000)
